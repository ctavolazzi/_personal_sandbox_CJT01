"""
Decision Matrix Utility
-----------------------
A comprehensive decision-making tool for quantifying and comparing options
against multiple criteria.

Usage:
    from tools.decision_matrix import make_decision

    result = make_decision(
        options=["Option A", "Option B", "Option C"],
        criteria=["Cost", "Speed", "Quality"],
        scores={
            "Option A": [7, 8, 6],
            "Option B": [9, 5, 7],
            "Option C": [6, 9, 8]
        },
        weights=[0.3, 0.2, 0.5]  # Optional: importance of each criterion
    )

    print(result)  # Shows ranking and analysis
"""

from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
import json


@dataclass
class DecisionResult:
    """Results from a decision matrix analysis."""

    winner: str
    rankings: List[Tuple[str, float]]
    scores_breakdown: Dict[str, Dict[str, float]]
    analysis_method: str
    total_score: Dict[str, float]
    normalized_scores: Dict[str, float] = field(default_factory=dict)
    confidence_score: float = 0.0
    recommendation: str = ""
    warnings: List[str] = field(default_factory=list)
    strengths: Dict[str, List[Tuple[str, float]]] = field(default_factory=dict)
    weaknesses: Dict[str, List[Tuple[str, float]]] = field(default_factory=dict)
    why_winner_won: str = ""
    top_n: Optional[int] = None

    def __str__(self) -> str:
        """Format results for display."""
        lines = [
            "=" * 70,
            f"DECISION MATRIX RESULTS ({self.analysis_method})",
            "=" * 70,
            f"\nðŸ† WINNER: {self.winner}",
            f"   Confidence: {self.confidence_score:.1f}%",
        ]

        if self.warnings:
            lines.append("\nâš ï¸  Warnings:")
            for warning in self.warnings:
                lines.append(f"   - {warning}")

        # Add "why winner won" explanation if available
        if self.why_winner_won:
            lines.append(f"\nâœ¨ WHY {self.winner.upper()} WON:")
            lines.append(f"   {self.why_winner_won}")

        lines.append(f"\nðŸ“Š RANKINGS:")

        # Show top N if specified, otherwise show all
        display_rankings = self.rankings[:self.top_n] if self.top_n else self.rankings

        for i, (option, score) in enumerate(display_rankings, 1):
            normalized = self.normalized_scores.get(option, 0)
            lines.append(
                f"   {i}. {option:20s} "
                f"Score: {score:6.2f} ({normalized:5.1f}%)"
            )

            # Show strengths and weaknesses if available
            if option in self.strengths and self.strengths[option]:
                top_strengths = self.strengths[option][:2]
                strength_str = ", ".join([f"{c} ({s:.1f})" for c, s in top_strengths])
                lines.append(f"      ðŸ’ª Strengths: {strength_str}")

            if option in self.weaknesses and self.weaknesses[option]:
                top_weaknesses = self.weaknesses[option][:2]
                weakness_str = ", ".join([f"{c} ({w:.1f})" for c, w in top_weaknesses])
                lines.append(f"      âš ï¸  Weaknesses: {weakness_str}")

        if self.top_n and len(self.rankings) > self.top_n:
            lines.append(f"   ... and {len(self.rankings) - self.top_n} more options")

        lines.extend(
            [
                f"\nðŸ’¡ RECOMMENDATION:",
                f"   {self.recommendation}",
                "\nðŸ“‹ DETAILED BREAKDOWN:",
            ]
        )

        for option, criteria_scores in self.scores_breakdown.items():
            lines.append(f"\n   {option}:")
            for criterion, score in criteria_scores.items():
                lines.append(f"      {criterion:20s}: {score:6.2f}")

        lines.append("=" * 70)
        return "\n".join(lines)

    def comparison_table(self) -> str:
        """Generate a side-by-side comparison table."""
        if not self.scores_breakdown:
            return "No breakdown available for comparison table."

        # Get all criteria from the first option
        first_option = list(self.scores_breakdown.keys())[0]
        criteria = list(self.scores_breakdown[first_option].keys())

        # Determine column widths
        option_width = max(len(opt) for opt in self.scores_breakdown.keys()) + 2
        criterion_width = max(len(c) for c in criteria) + 2

        # Build table
        lines = ["\nðŸ“Š COMPARISON TABLE:", "=" * 70]

        # Header row
        header = f"{'Criterion':<{criterion_width}} | "
        for option in self.scores_breakdown.keys():
            header += f"{option:^{option_width}} | "
        header += "Winner"
        lines.append(header)
        lines.append("-" * len(header))

        # Data rows
        for criterion in criteria:
            # Extract just the criterion name (remove weight info)
            clean_criterion = criterion.split(" (")[0]

            row = f"{clean_criterion:<{criterion_width}} | "

            # Get scores for this criterion across all options
            scores = {}
            for option in self.scores_breakdown.keys():
                score = self.scores_breakdown[option].get(criterion, 0)
                scores[option] = score
                row += f"{score:^{option_width}.1f} | "

            # Find winner for this criterion
            winner = max(scores, key=scores.get)
            row += f"{winner}"
            lines.append(row)

        lines.append("=" * 70)
        return "\n".join(lines)

    def to_dict(self) -> dict:
        """Convert to dictionary for serialization."""
        return {
            "winner": self.winner,
            "rankings": [(opt, round(float(score), 2)) for opt, score in self.rankings],
            "scores_breakdown": {
                opt: {crit: round(float(score), 2) for crit, score in crit_scores.items()}
                for opt, crit_scores in self.scores_breakdown.items()
            },
            "analysis_method": self.analysis_method,
            "total_score": {opt: round(float(score), 2) for opt, score in self.total_score.items()},
            "normalized_scores": {
                opt: round(float(score), 2) for opt, score in self.normalized_scores.items()
            },
            "confidence_score": round(self.confidence_score, 2),
            "recommendation": self.recommendation,
            "warnings": self.warnings,
            "strengths": {
                opt: [(c, round(float(s), 2)) for c, s in strengths]
                for opt, strengths in self.strengths.items()
            },
            "weaknesses": {
                opt: [(c, round(float(w), 2)) for c, w in weaknesses]
                for opt, weaknesses in self.weaknesses.items()
            },
            "why_winner_won": self.why_winner_won,
            "top_n": self.top_n,
        }

    def to_json(self, indent: int = 2) -> str:
        """Convert to JSON string."""
        return json.dumps(self.to_dict(), indent=indent)


class DecisionMatrix:
    """
    A flexible decision matrix for evaluating options against criteria.

    Supports multiple analysis methods:
    - weighted: Traditional weighted score (default)
    - normalized: Normalized scores (0-100 scale)
    - ranking: Convert scores to rankings per criterion
    - best_worst: Best-worst scaling method
    """

    def __init__(
        self,
        options: List[str],
        criteria: List[str],
        scores: Dict[str, List[Union[int, float]]],
        weights: Optional[List[float]] = None,
        method: str = "weighted",
    ):
        """
        Initialize decision matrix.

        Args:
            options: List of option names
            criteria: List of criterion names
            scores: Dict mapping option names to lists of scores
            weights: Optional list of weights (0-1) for each criterion
            method: Analysis method (weighted, normalized, ranking, best_worst)
        """
        self.options = options
        self.criteria = criteria
        self.scores = scores
        self.method = method

        # Validate inputs
        self._validate_inputs()

        # Set weights (equal if not provided)
        if weights is None:
            self.weights = [1.0 / len(criteria)] * len(criteria)
        else:
            # Normalize weights to sum to 1
            total = sum(weights)
            self.weights = [w / total for w in weights]

    def _validate_inputs(self):
        """Validate input data."""
        if not self.options:
            raise ValueError("Must provide at least one option")

        if not self.criteria:
            raise ValueError("Must provide at least one criterion")

        if not self.scores:
            raise ValueError("Must provide scores")

        # Check all options have scores
        for option in self.options:
            if option not in self.scores:
                raise ValueError(f"Missing scores for option: {option}")

            if len(self.scores[option]) != len(self.criteria):
                raise ValueError(
                    f"Option '{option}' has {len(self.scores[option])} "
                    f"scores but {len(self.criteria)} criteria"
                )

    def analyze(self) -> DecisionResult:
        """
        Perform decision matrix analysis.

        Returns:
            DecisionResult with rankings and analysis
        """
        if self.method == "weighted":
            return self._analyze_weighted()
        elif self.method == "normalized":
            return self._analyze_normalized()
        elif self.method == "ranking":
            return self._analyze_ranking()
        elif self.method == "best_worst":
            return self._analyze_best_worst()
        else:
            raise ValueError(f"Unknown analysis method: {self.method}")

    def _analyze_weighted(self) -> DecisionResult:
        """Traditional weighted score analysis."""
        total_scores = {}
        breakdown = {}

        # Calculate weighted scores
        for option in self.options:
            option_scores = self.scores[option]
            weighted_total = sum(
                score * weight
                for score, weight in zip(option_scores, self.weights)
            )
            total_scores[option] = weighted_total

            # Build breakdown
            breakdown[option] = {}
            for i, criterion in enumerate(self.criteria):
                weighted_score = option_scores[i] * self.weights[i]
                breakdown[option][f"{criterion} (w={self.weights[i]:.2f})"] = (
                    weighted_score
                )

        # Rank options
        rankings = sorted(
            total_scores.items(), key=lambda x: x[1], reverse=True
        )

        # Normalize scores to percentages
        max_score = max(total_scores.values())
        normalized = {
            opt: (score / max_score * 100) for opt, score in total_scores.items()
        }

        # Calculate improved confidence (gap between 1st and 2nd)
        if len(rankings) > 1:
            gap = rankings[0][1] - rankings[1][1]
            normalized_gap = normalized[rankings[0][0]] - normalized[rankings[1][0]]
            confidence = self._improve_confidence_score(
                gap, rankings[0][1], normalized_gap
            )
        else:
            confidence = 100.0

        # Calculate strengths and weaknesses
        strengths, weaknesses = self._calculate_strengths_weaknesses(breakdown)

        # Calculate why winner won
        why_winner_won = self._calculate_why_winner_won(
            rankings[0][0], rankings, breakdown
        )

        # Generate recommendation
        recommendation = self._generate_recommendation(
            rankings, normalized, confidence
        )
        warnings = self._generate_warnings(rankings, normalized)

        return DecisionResult(
            winner=rankings[0][0],
            rankings=rankings,
            scores_breakdown=breakdown,
            analysis_method="Weighted Score",
            total_score=total_scores,
            normalized_scores=normalized,
            confidence_score=confidence,
            recommendation=recommendation,
            warnings=warnings,
            strengths=strengths,
            weaknesses=weaknesses,
            why_winner_won=why_winner_won,
        )

    def _analyze_normalized(self) -> DecisionResult:
        """Normalized score analysis (0-100 scale per criterion)."""
        normalized_scores = {}
        breakdown = {}

        # Normalize each criterion to 0-100
        for criterion_idx, criterion in enumerate(self.criteria):
            criterion_scores = [
                self.scores[opt][criterion_idx] for opt in self.options
            ]
            min_score = min(criterion_scores)
            max_score = max(criterion_scores)
            score_range = max_score - min_score if max_score > min_score else 1

            for option in self.options:
                raw_score = self.scores[option][criterion_idx]
                normalized = ((raw_score - min_score) / score_range) * 100

                if option not in normalized_scores:
                    normalized_scores[option] = []
                normalized_scores[option].append(normalized)

        # Calculate weighted totals
        total_scores = {}
        for option in self.options:
            weighted_total = sum(
                score * weight
                for score, weight in zip(
                    normalized_scores[option], self.weights
                )
            )
            total_scores[option] = weighted_total

            # Build breakdown
            breakdown[option] = {}
            for i, criterion in enumerate(self.criteria):
                breakdown[option][f"{criterion} (normalized)"] = (
                    normalized_scores[option][i]
                )

        # Rank options
        rankings = sorted(
            total_scores.items(), key=lambda x: x[1], reverse=True
        )

        # Scores already normalized to 100
        normalized = {
            opt: (score / 100 * 100) for opt, score in total_scores.items()
        }

        # Calculate confidence
        if len(rankings) > 1:
            gap = rankings[0][1] - rankings[1][1]
            confidence = min(100, gap)
        else:
            confidence = 100.0

        recommendation = self._generate_recommendation(
            rankings, normalized, confidence
        )
        warnings = self._generate_warnings(rankings, normalized)

        return DecisionResult(
            winner=rankings[0][0],
            rankings=rankings,
            scores_breakdown=breakdown,
            analysis_method="Normalized Score (0-100)",
            total_score=total_scores,
            normalized_scores=normalized,
            confidence_score=confidence,
            recommendation=recommendation,
            warnings=warnings,
        )

    def _analyze_ranking(self) -> DecisionResult:
        """Ranking-based analysis (convert scores to rankings)."""
        total_ranks = {option: 0 for option in self.options}
        breakdown = {}

        # Convert scores to rankings for each criterion
        for criterion_idx, criterion in enumerate(self.criteria):
            criterion_scores = {
                opt: self.scores[opt][criterion_idx] for opt in self.options
            }
            ranked = sorted(
                criterion_scores.items(), key=lambda x: x[1], reverse=True
            )

            for rank, (option, score) in enumerate(ranked, 1):
                weighted_rank = rank * self.weights[criterion_idx]
                total_ranks[option] += weighted_rank

                if option not in breakdown:
                    breakdown[option] = {}
                breakdown[option][f"{criterion} (rank)"] = rank

        # Lower rank is better, so reverse
        rankings = sorted(total_ranks.items(), key=lambda x: x[1])

        # Normalize (invert since lower is better)
        max_rank = max(total_ranks.values())
        normalized = {
            opt: ((max_rank - rank) / max_rank * 100)
            for opt, rank in total_ranks.items()
        }

        if len(rankings) > 1:
            gap = rankings[1][1] - rankings[0][1]
            confidence = min(100, (gap / max_rank) * 100)
        else:
            confidence = 100.0

        recommendation = self._generate_recommendation(
            [(opt, normalized[opt]) for opt, _ in rankings],
            normalized,
            confidence,
        )
        warnings = self._generate_warnings(rankings, normalized)

        return DecisionResult(
            winner=rankings[0][0],
            rankings=[(opt, normalized[opt]) for opt, _ in rankings],
            scores_breakdown=breakdown,
            analysis_method="Ranking Method",
            total_score={opt: normalized[opt] for opt, _ in rankings},
            normalized_scores=normalized,
            confidence_score=confidence,
            recommendation=recommendation,
            warnings=warnings,
        )

    def _analyze_best_worst(self) -> DecisionResult:
        """Best-worst scaling method."""
        scaled_scores = {}
        breakdown = {}

        # Scale each option relative to best and worst
        for option in self.options:
            option_scores = self.scores[option]
            min_possible = min(option_scores)
            max_possible = max(option_scores)

            # Calculate how close to best vs worst
            total_score = 0
            breakdown[option] = {}

            for i, criterion in enumerate(self.criteria):
                score = option_scores[i]
                criterion_scores = [
                    self.scores[opt][i] for opt in self.options
                ]
                best = max(criterion_scores)
                worst = min(criterion_scores)

                if best > worst:
                    scaled = (score - worst) / (best - worst)
                else:
                    scaled = 1.0

                weighted_scaled = scaled * self.weights[i]
                total_score += weighted_scaled

                breakdown[option][f"{criterion} (best-worst)"] = scaled * 100

            scaled_scores[option] = total_score

        # Rank options
        rankings = sorted(
            scaled_scores.items(), key=lambda x: x[1], reverse=True
        )

        # Normalize to percentages
        max_score = max(scaled_scores.values())
        normalized = {
            opt: (score / max_score * 100)
            for opt, score in scaled_scores.items()
        }

        if len(rankings) > 1:
            gap = rankings[0][1] - rankings[1][1]
            confidence = min(100, (gap / rankings[0][1]) * 100)
        else:
            confidence = 100.0

        recommendation = self._generate_recommendation(
            rankings, normalized, confidence
        )
        warnings = self._generate_warnings(rankings, normalized)

        return DecisionResult(
            winner=rankings[0][0],
            rankings=rankings,
            scores_breakdown=breakdown,
            analysis_method="Best-Worst Scaling",
            total_score=scaled_scores,
            normalized_scores=normalized,
            confidence_score=confidence,
            recommendation=recommendation,
            warnings=warnings,
        )

    def _calculate_strengths_weaknesses(
        self, breakdown: Dict[str, Dict[str, float]]
    ) -> Tuple[Dict[str, List[Tuple[str, float]]], Dict[str, List[Tuple[str, float]]]]:
        """Calculate top strengths and weaknesses for each option."""
        strengths = {}
        weaknesses = {}

        for option in breakdown:
            # Get scores for this option
            criterion_scores = []
            for criterion, score in breakdown[option].items():
                # Extract clean criterion name
                clean_criterion = criterion.split(" (")[0]
                criterion_scores.append((clean_criterion, score))

            # Sort by score
            criterion_scores.sort(key=lambda x: x[1], reverse=True)

            top_strengths = criterion_scores[:3]
            strength_names = {name for name, _ in top_strengths}

            bottom_candidates = list(reversed(criterion_scores))
            filtered_weaknesses = []
            for name, score in bottom_candidates:
                if name in strength_names:
                    continue
                filtered_weaknesses.append((name, score))
                if len(filtered_weaknesses) == 3:
                    break

            strengths[option] = top_strengths
            weaknesses[option] = filtered_weaknesses

        return strengths, weaknesses

    def _calculate_why_winner_won(
        self,
        winner: str,
        rankings: List[Tuple[str, float]],
        breakdown: Dict[str, Dict[str, float]],
    ) -> str:
        """Generate explanation for why the winner won."""
        if len(rankings) < 2:
            return f"Only option available."

        runner_up = rankings[1][0]

        # Find criteria where winner significantly outperformed runner-up
        winner_scores = breakdown[winner]
        runner_up_scores = breakdown[runner_up]

        advantages = []
        for criterion in winner_scores:
            clean_criterion = criterion.split(" (")[0]
            winner_score = winner_scores[criterion]
            runner_up_score = runner_up_scores.get(criterion, 0)

            if winner_score > runner_up_score * 1.1:  # 10% better
                advantage = winner_score - runner_up_score
                # Extract weight if present
                weight = None
                if "(w=" in criterion:
                    weight_str = criterion.split("(w=")[1].split(")")[0]
                    weight = float(weight_str)

                advantages.append((clean_criterion, advantage, weight))

        if not advantages:
            return f"Marginally better overall performance across all criteria."

        # Sort by advantage
        advantages.sort(key=lambda x: x[1], reverse=True)

        # Generate explanation
        top_advantages = advantages[:2]
        parts = []
        for criterion, adv, weight in top_advantages:
            if weight:
                parts.append(f"{criterion} ({weight:.0%} weight, +{adv:.1f} points)")
            else:
                parts.append(f"{criterion} (+{adv:.1f} points)")

        return f"Excelled in {' and '.join(parts)}"

    def _improve_confidence_score(
        self,
        gap: float,
        winner_score: float,
        normalized_gap: float = None
    ) -> float:
        """
        Calculate improved confidence score.

        Uses both absolute gap and relative gap for better calibration.
        """
        # Relative gap (original method, but scaled up)
        relative_conf = (gap / winner_score) * 100

        # Normalized gap (if provided, from normalized scores)
        if normalized_gap is not None:
            # For normalized scores, a 20 point gap should be high confidence
            normalized_conf = min(100, normalized_gap * 2.5)
            # Blend the two approaches
            confidence = (relative_conf * 0.4) + (normalized_conf * 0.6)
        else:
            # Scale up relative confidence by 1.5x to be less conservative
            confidence = min(100, relative_conf * 1.5)

        return confidence

    def _generate_recommendation(
        self,
        rankings: List[Tuple[str, float]],
        normalized: Dict[str, float],
        confidence: float,
    ) -> str:
        """Generate a recommendation based on results."""
        winner = rankings[0][0]
        winner_score = normalized[winner]

        # Adjusted thresholds (lowered from 70/40 to 55/30)
        if confidence > 55:
            return (
                f"Strong recommendation: '{winner}' clearly outperforms "
                f"other options with {winner_score:.1f}% score."
            )
        elif confidence > 30:
            if len(rankings) > 1:
                runner_up = rankings[1][0]
                return (
                    f"Moderate recommendation: '{winner}' is best "
                    f"({winner_score:.1f}%), but '{runner_up}' "
                    f"({normalized[runner_up]:.1f}%) is competitive. "
                    f"Consider other factors."
                )
            return f"Moderate recommendation: '{winner}' with {winner_score:.1f}% score."
        else:
            top_3 = [opt for opt, _ in rankings[:3]]
            return (
                f"Weak recommendation: Options are closely matched. "
                f"Top choices: {', '.join(top_3)}. "
                f"Consider additional criteria or stakeholder input."
            )

    def _generate_warnings(
        self,
        rankings: List[Tuple[str, float]],
        normalized: Dict[str, float],
    ) -> List[str]:
        """Create human-readable warnings for borderline scenarios."""
        warnings: List[str] = []

        if len(rankings) > 1:
            winner = rankings[0][0]
            runner_up = rankings[1][0]
            winner_score = normalized.get(winner, rankings[0][1])
            runner_score = normalized.get(runner_up, rankings[1][1])
            gap = winner_score - runner_score

            if gap < 5:
                warnings.append(
                    f"Scores are within {gap:.1f} points between '{winner}' and "
                    f"'{runner_up}' â€” treat as a statistical tie."
                )

        return warnings


def make_decision(
    options: List[str],
    criteria: List[str],
    scores: Dict[str, List[Union[int, float]]],
    weights: Optional[List[float]] = None,
    method: str = "weighted",
    show_all_methods: bool = False,
    top_n: Optional[int] = None,
) -> Union[DecisionResult, Dict[str, DecisionResult]]:
    """
    Make a decision using a decision matrix.

    Args:
        options: List of option names to compare
        criteria: List of criteria to evaluate against
        scores: Dictionary mapping option names to lists of scores
        weights: Optional weights for each criterion (defaults to equal)
        method: Analysis method - 'weighted', 'normalized', 'ranking', 'best_worst'
        show_all_methods: If True, run all methods and return comparison
        top_n: Optional limit on number of options to display (shows top N only)

    Returns:
        DecisionResult with rankings and analysis, or dict of all methods if
        show_all_methods=True

    Example:
        >>> result = make_decision(
        ...     options=["Python", "JavaScript", "Go"],
        ...     criteria=["Learning Curve", "Performance", "Community"],
        ...     scores={
        ...         "Python": [9, 7, 10],
        ...         "JavaScript": [8, 6, 9],
        ...         "Go": [6, 9, 7]
        ...     },
        ...     weights=[0.3, 0.4, 0.3]
        ... )
        >>> print(result)
    """
    if show_all_methods:
        results = {}
        for method_name in ["weighted", "normalized", "ranking", "best_worst"]:
            matrix = DecisionMatrix(
                options, criteria, scores, weights, method_name
            )
            result = matrix.analyze()
            result.top_n = top_n
            results[method_name] = result
        return results
    else:
        matrix = DecisionMatrix(options, criteria, scores, weights, method)
        result = matrix.analyze()
        result.top_n = top_n
        return result


def compare_methods(
    options: List[str],
    criteria: List[str],
    scores: Dict[str, List[Union[int, float]]],
    weights: Optional[List[float]] = None,
) -> str:
    """
    Compare results across all analysis methods.

    Returns a formatted comparison of all methods.
    """
    results = make_decision(
        options, criteria, scores, weights, show_all_methods=True
    )

    lines = [
        "=" * 70,
        "DECISION MATRIX - METHOD COMPARISON",
        "=" * 70,
    ]

    for method_name, result in results.items():
        lines.append(f"\n{method_name.upper()} METHOD:")
        lines.append(f"  Winner: {result.winner}")
        lines.append(f"  Confidence: {result.confidence_score:.1f}%")
        lines.append("  Top 3:")
        for i, (opt, score) in enumerate(result.rankings[:3], 1):
            lines.append(f"    {i}. {opt}: {score:.2f}")

    # Consensus check
    lines.append("\n" + "=" * 70)
    winners = [result.winner for result in results.values()]
    winner_counts = {w: winners.count(w) for w in set(winners)}
    consensus = max(winner_counts.items(), key=lambda x: x[1])

    lines.append(f"CONSENSUS: {consensus[0]} ({consensus[1]}/4 methods)")
    lines.append("=" * 70)

    return "\n".join(lines)
